{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daa1bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ac4c8",
   "metadata": {},
   "source": [
    "## LLM-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe168aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Dataset_Books.csv\")\n",
    "df2 = pd.read_csv(\"Dataset_Movies_and_TV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ac2e732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>head_id</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>tail_id</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>591833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:author</td>\n",
       "      <td>res:John_Shaw</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:author</td>\n",
       "      <td>res:John_Shaw</td>\n",
       "      <td>591833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:composer</td>\n",
       "      <td>res:B._F._White</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id  rating                                       entity_id  \\\n",
       "0  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "1  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "2  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "3  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "4  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "\n",
       "                                          head_id   relation_id  \\\n",
       "0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   dct:subject   \n",
       "1  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   dct:subject   \n",
       "2  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal    dbo:author   \n",
       "3  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal    dbo:author   \n",
       "4  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal  dbo:composer   \n",
       "\n",
       "           tail_id  reviewerID  \n",
       "0       cat:Hymnal      366595  \n",
       "1       cat:Hymnal      591833  \n",
       "2    res:John_Shaw      366595  \n",
       "3    res:John_Shaw      591833  \n",
       "4  res:B._F._White      366595  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f6ac84d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>head_id</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>tail_id</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dbo:director</td>\n",
       "      <td>res:Michael_Merriman</td>\n",
       "      <td>586297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dbo:director</td>\n",
       "      <td>res:Michael_Merriman</td>\n",
       "      <td>226404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dbo:director</td>\n",
       "      <td>res:Michael_Merriman</td>\n",
       "      <td>178001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dbo:director</td>\n",
       "      <td>res:Michael_Merriman</td>\n",
       "      <td>388575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dbo:director</td>\n",
       "      <td>res:Michael_Merriman</td>\n",
       "      <td>515515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id  rating                                          entity_id  \\\n",
       "0  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "1  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "2  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "3  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "4  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "\n",
       "                                             head_id   relation_id  \\\n",
       "0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dbo:director   \n",
       "1  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dbo:director   \n",
       "2  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dbo:director   \n",
       "3  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dbo:director   \n",
       "4  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dbo:director   \n",
       "\n",
       "                tail_id  reviewerID  \n",
       "0  res:Michael_Merriman      586297  \n",
       "1  res:Michael_Merriman      226404  \n",
       "2  res:Michael_Merriman      178001  \n",
       "3  res:Michael_Merriman      388575  \n",
       "4  res:Michael_Merriman      515515  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17192897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset\n",
    "df_books = pd.read_csv(\"Dataset_Books.csv\")\n",
    "df_movies = pd.read_csv(\"Dataset_Movies_and_TV.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a7f46e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom di df_books: Index(['item_id', 'rating', 'entity_id', 'head_id', 'relation_id', 'tail_id',\n",
      "       'reviewerID'],\n",
      "      dtype='object')\n",
      "Kolom di df_movies: Index(['item_id', 'rating', 'entity_id', 'head_id', 'relation_id', 'tail_id',\n",
      "       'reviewerID'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Kolom di df_books:\", df_books.columns)\n",
    "print(\"Kolom di df_movies:\", df_movies.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06180926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 831820 entries, 0 to 831819\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   item_id      831820 non-null  object \n",
      " 1   rating       831820 non-null  float64\n",
      " 2   entity_id    831820 non-null  object \n",
      " 3   head_id      831820 non-null  object \n",
      " 4   relation_id  831820 non-null  object \n",
      " 5   tail_id      831820 non-null  object \n",
      " 6   reviewerID   831820 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 44.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e5a52373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 934345 entries, 0 to 934344\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   item_id      934345 non-null  object \n",
      " 1   rating       934345 non-null  float64\n",
      " 2   entity_id    934345 non-null  object \n",
      " 3   head_id      934345 non-null  object \n",
      " 4   relation_id  934345 non-null  object \n",
      " 5   tail_id      934345 non-null  object \n",
      " 6   reviewerID   934345 non-null  int64  \n",
      "dtypes: float64(1), int64(1), object(5)\n",
      "memory usage: 49.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_movies.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6121e2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Memuat dataset\n",
    "# df_books2 = pd.read_csv(\"Dataset_Books_GEMINI.csv\")\n",
    "# df_movies2 = pd.read_csv(\"Dataset_Movies_and_TV_GEMINI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9133981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Kolom di df_books:\", df_books2.columns)\n",
    "# print(\"Kolom di df_movies:\", df_movies2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74a15ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_books2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e414128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_movies2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b04f2d",
   "metadata": {},
   "source": [
    "## Cross-Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae33b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # ==========================#\n",
    "# # Konfigurasi File\n",
    "# # ==========================#\n",
    "\n",
    "# movies_path = \"Dataset_Movies_and_TV.csv\"\n",
    "# books_path = \"Dataset_Books.csv\"\n",
    "\n",
    "# # ==========================#\n",
    "# # Load dan Bersihkan Data\n",
    "# # ==========================#\n",
    "\n",
    "# # Load dataset dengan reviewerID\n",
    "# df_movies = pd.read_csv(movies_path, usecols=[\"reviewerID\", \"item_id\", \"head_id\", \"tail_id\"])\n",
    "# df_books = pd.read_csv(books_path, usecols=[\"reviewerID\", \"item_id\", \"head_id\", \"tail_id\"])\n",
    "\n",
    "# # Bersihkan tail_id dari prefix\n",
    "# df_movies[\"tail_id\"] = df_movies[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "# df_books[\"tail_id\"] = df_books[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "\n",
    "# # Hilangkan duplikat agar lebih efisien\n",
    "# df_movies = df_movies.drop_duplicates(subset=[\"reviewerID\", \"item_id\", \"head_id\", \"tail_id\"])\n",
    "# df_books = df_books.drop_duplicates(subset=[\"reviewerID\", \"item_id\", \"head_id\", \"tail_id\"])\n",
    "\n",
    "# # Cari tail_id yang muncul di kedua domain\n",
    "# common_tail_ids = set(df_movies[\"tail_id\"]).intersection(set(df_books[\"tail_id\"]))\n",
    "\n",
    "# # Ambil 1 baris per tail_id dari masing-masing domain\n",
    "# sampled_movies = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)].groupby(\"tail_id\").apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "# sampled_books = df_books[df_books[\"tail_id\"].isin(common_tail_ids)].groupby(\"tail_id\").apply(lambda x: x.sample(1)).reset_index(drop=True)\n",
    "\n",
    "# # Merge berdasarkan tail_id\n",
    "# merged_df = pd.merge(\n",
    "#     sampled_movies,\n",
    "#     sampled_books,\n",
    "#     on=\"tail_id\",\n",
    "#     suffixes=(\"_movies\", \"_books\")\n",
    "# )\n",
    "\n",
    "# # Ambil dan ubah nama kolom\n",
    "# final_df = merged_df[[\n",
    "#     \"reviewerID_movies\", \"item_id_movies\", \"head_id_movies\",\n",
    "#     \"tail_id\", \"head_id_books\", \"item_id_books\", \"reviewerID_books\"\n",
    "# ]].rename(columns={\n",
    "#     \"reviewerID_movies\": \"User_id_Movie\",\n",
    "#     \"item_id_movies\": \"Movie_id\",\n",
    "#     \"head_id_movies\": \"Judul_Film\",\n",
    "#     \"tail_id\": \"Relation\",\n",
    "#     \"head_id_books\": \"Judul_Buku\",\n",
    "#     \"item_id_books\": \"Book_id\",\n",
    "#     \"reviewerID_books\": \"User_id_Book\"\n",
    "# })\n",
    "\n",
    "# # ==========================#\n",
    "# # Output\n",
    "# # ==========================#\n",
    "\n",
    "# print(final_df.head())  # Cetak beberapa baris untuk cek\n",
    "# final_df.to_csv(\"Movies_Books_Sample_Merge.csv\", index=False)\n",
    "# print(f\"\\n✅ Ditemukan {len(final_df)} koneksi antar domain (satu kombinasi unik per tail_id).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a110916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      reviewerID_Movie    Movie_id  \\\n",
      "0               317296  GYYTMMBWGA   \n",
      "1               317296  GYYTMMBWGA   \n",
      "2               317296  GYYTMMBWGA   \n",
      "3               317296  GYYTMMBWGA   \n",
      "4               166595  GYZTAMRZGI   \n",
      "...                ...         ...   \n",
      "2192            157110  GA4TQOBWGY   \n",
      "2193            157110  GA4TQOBWGY   \n",
      "2194            160631  GE3DGMZUGI   \n",
      "2195            115321  GYZTAMJWGU   \n",
      "2196             57922  GE2TMMRQGI   \n",
      "\n",
      "                                             Judul_Film              Relation  \\\n",
      "0     res:Pasta_(Award_Winning_Drama)_(Korean_Tv_Dra...                         \n",
      "1     res:Pasta_(Award_Winning_Drama)_(Korean_Tv_Dra...                         \n",
      "2     res:Pasta_(Award_Winning_Drama)_(Korean_Tv_Dra...                         \n",
      "3     res:Pasta_(Award_Winning_Drama)_(Korean_Tv_Dra...                         \n",
      "4                              res:Young_Tom_Edison_VHS         Thomas_Edison   \n",
      "...                                                 ...                   ...   \n",
      "2192  res:Behold_Your_God:_Rethinking_God_Biblically...  Christian_Book_Award   \n",
      "2193  res:Behold_Your_God:_Rethinking_God_Biblically...  Christian_Book_Award   \n",
      "2194  res:When_Sinners_Say_&quot;I_Do&quot;_Video_Se...  Christian_Book_Award   \n",
      "2195          res:Rona_Jaffe's_Mazes_&amp;_Monsters_VHS       George_Eckstein   \n",
      "2196                          res:Margaret's_Museum_VHS           Ashley_Judd   \n",
      "\n",
      "                                             Judul_Buku     Book_id  \\\n",
      "0                                      res:Post_Captain  GY4GGZJRHE   \n",
      "1     res:Core_of_the_Yoga_Sutras:_The_Definitive_Gu...  GRQTEZDEMY   \n",
      "2                       res:The_Spice_Islands_Cook_Book  MRSDANJVMY   \n",
      "3     res:Hidden_Cities:_The_Discovery_and_Loss_of_A...  GEZDMNDBGA   \n",
      "4     res:Thomas_Edison:_Young_Inventor_(Childhood_o...  MYZWKNJSMM   \n",
      "...                                                 ...         ...   \n",
      "2192  res:The_Scandalous_Gospel_of_Jesus:_What's_So_...  MQ4DOYZWHB   \n",
      "2193  res:Praying_Like_Jesus:_The_Lord's_Prayer_in_a...  MRRGCMJTGJ   \n",
      "2194                            res:Unlocking_the_Bible  MU4TOZLFGI   \n",
      "2195                           res:Memories_of_Midnight  MFSDCM3BGJ   \n",
      "2196                             res:Where_the_Heart_Is  MZSWCYRQGV   \n",
      "\n",
      "      reviewerID_Book  \n",
      "0              293577  \n",
      "1              463537  \n",
      "2              133005  \n",
      "3               86175  \n",
      "4               86000  \n",
      "...               ...  \n",
      "2192           371657  \n",
      "2193           296289  \n",
      "2194           318192  \n",
      "2195           168360  \n",
      "2196           220535  \n",
      "\n",
      "[2197 rows x 7 columns]\n",
      "\n",
      "✅ Ditemukan 2197 koneksi antar domain berdasarkan relasi yang identik!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==========================#\n",
    "# Konfigurasi File\n",
    "# ==========================#\n",
    "\n",
    "movies_path = \"Dataset_Movies_and_TV.csv\"\n",
    "books_path = \"Dataset_Books.csv\"\n",
    "MAX_PAIRS_PER_TAIL = 5  # Atur sesuai kebutuhan\n",
    "\n",
    "# ==========================#\n",
    "# Load dan Bersihkan Data\n",
    "# ==========================#\n",
    "\n",
    "# Load dataset\n",
    "df_movies = pd.read_csv(movies_path, usecols=[\"item_id\", \"head_id\", \"tail_id\", \"reviewerID\"])\n",
    "df_books = pd.read_csv(books_path, usecols=[\"item_id\", \"head_id\", \"tail_id\", \"reviewerID\"])\n",
    "\n",
    "# Hilangkan prefix di tail_id\n",
    "df_movies[\"tail_id\"] = df_movies[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "df_books[\"tail_id\"] = df_books[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "\n",
    "# Hilangkan duplikat\n",
    "df_movies = df_movies.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "df_books = df_books.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "\n",
    "# Cari tail_id yang muncul di kedua domain\n",
    "common_tail_ids = set(df_movies[\"tail_id\"]).intersection(set(df_books[\"tail_id\"]))\n",
    "\n",
    "# Filter berdasarkan relasi yang sama\n",
    "df_movies_filtered = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "df_books_filtered = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# Kelompokkan berdasarkan tail_id\n",
    "grouped_movies = df_movies_filtered.groupby(\"tail_id\")\n",
    "grouped_books = df_books_filtered.groupby(\"tail_id\")\n",
    "\n",
    "# Siapkan struktur data untuk hasil\n",
    "result_rows = []\n",
    "used_pairs = set()\n",
    "\n",
    "# Loop melalui setiap tail_id yang sama\n",
    "for tail in common_tail_ids:\n",
    "    if tail not in grouped_books.groups or tail not in grouped_movies.groups:\n",
    "        continue\n",
    "\n",
    "    books_group = grouped_books.get_group(tail)\n",
    "    movies_group = grouped_movies.get_group(tail)\n",
    "\n",
    "    count = 0  # penghitung untuk setiap tail_id\n",
    "    for _, movie_row in movies_group.iterrows():\n",
    "        for _, book_row in books_group.iterrows():\n",
    "            user_pair = (movie_row[\"reviewerID\"], book_row[\"reviewerID\"])\n",
    "            if user_pair not in used_pairs:\n",
    "                used_pairs.add(user_pair)\n",
    "                result_rows.append({\n",
    "                    \"reviewerID_Movie\": movie_row[\"reviewerID\"],\n",
    "                    \"Movie_id\": movie_row[\"item_id\"],\n",
    "                    \"Judul_Film\": movie_row[\"head_id\"],\n",
    "                    \"Relation\": tail,\n",
    "                    \"Judul_Buku\": book_row[\"head_id\"],\n",
    "                    \"Book_id\": book_row[\"item_id\"],\n",
    "                    \"reviewerID_Book\": book_row[\"reviewerID\"]\n",
    "                })\n",
    "                count += 1\n",
    "                if count >= MAX_PAIRS_PER_TAIL:\n",
    "                    break\n",
    "        if count >= MAX_PAIRS_PER_TAIL:\n",
    "            break\n",
    "\n",
    "# Buat DataFrame hasil akhir\n",
    "final_df = pd.DataFrame(result_rows)\n",
    "\n",
    "# ==========================#\n",
    "# Output\n",
    "# ==========================#\n",
    "\n",
    "print(final_df)\n",
    "final_df.to_csv(\"Movies_Books_Manual_Merge(2).csv\", index=False)\n",
    "print(f\"\\n✅ Ditemukan {len(final_df)} koneksi antar domain berdasarkan relasi yang identik!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27a4587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # ======================= #\n",
    "# # 1. Load Dataset\n",
    "# # ======================= #\n",
    "# # Gantilah path di bawah ini sesuai file CSV kamu\n",
    "# df_movies = pd.read_csv(\"Dataset_Movies_and_TV.csv\")\n",
    "# df_books = pd.read_csv(\"Dataset_Books.csv\")\n",
    "\n",
    "# # ======================= #\n",
    "# # 2. Filter Tail ID yang Sama\n",
    "# # ======================= #\n",
    "# common_tail_ids = set(df_books[\"tail_id\"]).intersection(df_movies[\"tail_id\"])\n",
    "\n",
    "# df_books = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "# df_movies = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# # ======================= #\n",
    "# # 3. Grouping\n",
    "# # ======================= #\n",
    "# grouped_books = df_books.groupby(\"tail_id\")\n",
    "# grouped_movies = df_movies.groupby(\"tail_id\")\n",
    "\n",
    "# # ======================= #\n",
    "# # 4. Generate Unique User Cross-Domain Pairs\n",
    "# # ======================= #\n",
    "# result_rows = []\n",
    "# used_pairs = set()\n",
    "\n",
    "# for tail in common_tail_ids:\n",
    "#     books_group = grouped_books.get_group(tail)\n",
    "#     movies_group = grouped_movies.get_group(tail)\n",
    "\n",
    "#     for _, movie_row in movies_group.iterrows():\n",
    "#         for _, book_row in books_group.iterrows():\n",
    "#             user_pair = (movie_row[\"reviewerID\"], book_row[\"reviewerID\"])\n",
    "#             if user_pair not in used_pairs:\n",
    "#                 used_pairs.add(user_pair)\n",
    "#                 result_rows.append({\n",
    "#                     \"reviewerID_Movie\": movie_row[\"reviewerID\"],\n",
    "#                     \"Movie_id\": movie_row[\"item_id\"],\n",
    "#                     \"Judul_Film\": movie_row[\"head_id\"],\n",
    "#                     \"Relation\": tail,\n",
    "#                     \"Judul_Buku\": book_row[\"head_id\"],\n",
    "#                     \"Book_id\": book_row[\"item_id\"],\n",
    "#                     \"reviewerID_Book\": book_row[\"reviewerID\"]\n",
    "#                 })\n",
    "\n",
    "# # ======================= #\n",
    "# # 5. Export as CSV\n",
    "# # ======================= #\n",
    "# final_df = pd.DataFrame(result_rows)\n",
    "# final_df.to_csv(\"Movie_Book_Merge.csv\", index=False)\n",
    "\n",
    "# print(f\"✅ Done! Total unique cross-domain user pairs: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210dc7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3680\\2988030106.py:7: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_books = pd.read_csv(\"Dataset_Books_GEMINI.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Total unique cross-domain user pairs: 4266186\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ======================= #\n",
    "# 1. Load Dataset\n",
    "# ======================= #\n",
    "df_movies = pd.read_csv(\"Dataset_Movies_and_TV_GEMINI.csv\")\n",
    "df_books = pd.read_csv(\"Dataset_Books_GEMINI.csv\")\n",
    "\n",
    "# ======================= #\n",
    "# 2. Filter Tail ID yang Sama\n",
    "# ======================= #\n",
    "common_tail_ids = set(df_books[\"tail_id\"]).intersection(df_movies[\"tail_id\"])\n",
    "\n",
    "df_books = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "df_movies = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# ======================= #\n",
    "# 3. Grouping\n",
    "# ======================= #\n",
    "grouped_books = df_books.groupby(\"tail_id\")\n",
    "grouped_movies = df_movies.groupby(\"tail_id\")\n",
    "\n",
    "# ======================= #\n",
    "# 4. Generate Unique User Cross-Domain Pairs\n",
    "# ======================= #\n",
    "MAX_PAIRS_PER_TAIL = 100000  # batas maksimal pasangan per tail_id\n",
    "result_rows = []\n",
    "used_pairs = set()\n",
    "\n",
    "for tail in common_tail_ids:\n",
    "    books_group = grouped_books.get_group(tail)\n",
    "    movies_group = grouped_movies.get_group(tail)\n",
    "\n",
    "    count = 0  # penghitung untuk setiap tail_id\n",
    "    for _, movie_row in movies_group.iterrows():\n",
    "        for _, book_row in books_group.iterrows():\n",
    "            user_pair = (movie_row[\"reviewerID\"], book_row[\"reviewerID\"])\n",
    "            if user_pair not in used_pairs:\n",
    "                used_pairs.add(user_pair)\n",
    "                result_rows.append({\n",
    "                    \"reviewerID_Movie\": movie_row[\"reviewerID\"],\n",
    "                    \"Movie_id\": movie_row[\"item_id\"],\n",
    "                    \"Judul_Film\": movie_row[\"head_id\"],\n",
    "                    \"Relation\": tail,\n",
    "                    \"Judul_Buku\": book_row[\"head_id\"],\n",
    "                    \"Book_id\": book_row[\"item_id\"],\n",
    "                    \"reviewerID_Book\": book_row[\"reviewerID\"]\n",
    "                })\n",
    "                count += 1\n",
    "                if count >= MAX_PAIRS_PER_TAIL:\n",
    "                    break\n",
    "        if count >= MAX_PAIRS_PER_TAIL:\n",
    "            break\n",
    "\n",
    "# ======================= #\n",
    "# 5. Export as CSV\n",
    "# ======================= #\n",
    "final_df = pd.DataFrame(result_rows)\n",
    "final_df.to_csv(\"Movie_Book_Merge_GEMINI.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Total unique cross-domain user pairs: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66e5662a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Jumlah User Unik      : 34969\n",
      "🧮 Jumlah Item Unik      : 961\n",
      "✅ Jumlah Interaksi      : 1043830\n",
      "📉 Sparsity              : 0.968938 (atau 96.89%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ======================= #\n",
    "# 1. Load Hasil Dataset\n",
    "# ======================= #\n",
    "df_M = pd.read_csv(\"Movie_Book_Merge_GEMINI.csv\")\n",
    "\n",
    "# ======================= #\n",
    "# 2. Hitung Unique Users dan Items\n",
    "# ======================= #\n",
    "unique_users = pd.unique(df_M[[\"reviewerID_Movie\", \"reviewerID_Book\"]].values.ravel())\n",
    "unique_items = pd.unique(df_M[[\"Movie_id\", \"Book_id\"]].values.ravel())\n",
    "\n",
    "num_users = len(unique_users)\n",
    "num_items = len(unique_items)\n",
    "num_interactions = len(df_M)\n",
    "\n",
    "# ======================= #\n",
    "# 3. Hitung Sparsity\n",
    "# ======================= #\n",
    "possible_interactions = num_users * num_items\n",
    "sparsity = 1 - (num_interactions / possible_interactions)\n",
    "\n",
    "print(f\"🧮 Jumlah User Unik      : {num_users}\")\n",
    "print(f\"🧮 Jumlah Item Unik      : {num_items}\")\n",
    "print(f\"✅ Jumlah Interaksi      : {num_interactions}\")\n",
    "print(f\"📉 Sparsity              : {sparsity:.6f} (atau {sparsity*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ea163b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done! Total unique cross-domain user pairs: 66490\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ======================= #\n",
    "# 1. Load Dataset\n",
    "# ======================= #\n",
    "df_movies = pd.read_csv(\"Dataset_Movies_and_TV.csv\")\n",
    "df_books = pd.read_csv(\"Dataset_Books.csv\")\n",
    "\n",
    "# ======================= #\n",
    "# 2. Filter Tail ID yang Sama\n",
    "# ======================= #\n",
    "common_tail_ids = set(df_books[\"tail_id\"]).intersection(df_movies[\"tail_id\"])\n",
    "\n",
    "df_books = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "df_movies = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# ======================= #\n",
    "# 3. Grouping\n",
    "# ======================= #\n",
    "grouped_books = df_books.groupby(\"tail_id\")\n",
    "grouped_movies = df_movies.groupby(\"tail_id\")\n",
    "\n",
    "# ======================= #\n",
    "# 4. Generate Unique User Cross-Domain Pairs\n",
    "# ======================= #\n",
    "MAX_PAIRS_PER_TAIL = 100  # batas maksimal pasangan per tail_id\n",
    "result_rows = []\n",
    "used_pairs = set()\n",
    "\n",
    "for tail in common_tail_ids:\n",
    "    books_group = grouped_books.get_group(tail)\n",
    "    movies_group = grouped_movies.get_group(tail)\n",
    "\n",
    "    count = 0  # penghitung untuk setiap tail_id\n",
    "    for _, movie_row in movies_group.iterrows():\n",
    "        for _, book_row in books_group.iterrows():\n",
    "            user_pair = (movie_row[\"reviewerID\"], book_row[\"reviewerID\"])\n",
    "            if user_pair not in used_pairs:\n",
    "                used_pairs.add(user_pair)\n",
    "                result_rows.append({\n",
    "                    \"reviewerID_Movie\": movie_row[\"reviewerID\"],\n",
    "                    \"Movie_id\": movie_row[\"item_id\"],\n",
    "                    \"Judul_Film\": movie_row[\"head_id\"],\n",
    "                    \"Relation\": tail,\n",
    "                    \"Judul_Buku\": book_row[\"head_id\"],\n",
    "                    \"Book_id\": book_row[\"item_id\"],\n",
    "                    \"reviewerID_Book\": book_row[\"reviewerID\"]\n",
    "                })\n",
    "                count += 1\n",
    "                if count >= MAX_PAIRS_PER_TAIL:\n",
    "                    break\n",
    "        if count >= MAX_PAIRS_PER_TAIL:\n",
    "            break\n",
    "\n",
    "# ======================= #\n",
    "# 5. Export as CSV\n",
    "# ======================= #\n",
    "final_df = pd.DataFrame(result_rows)\n",
    "final_df.to_csv(\"Movie_Book_Merge.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Done! Total unique cross-domain user pairs: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5c18e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧮 Jumlah User Unik      : 22570\n",
      "🧮 Jumlah Item Unik      : 1830\n",
      "✅ Jumlah Interaksi      : 66490\n",
      "📉 Sparsity              : 0.998390 (atau 99.84%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ======================= #\n",
    "# 1. Load Hasil Dataset\n",
    "# ======================= #\n",
    "df = pd.read_csv(\"Movie_Book_Merge.csv\")\n",
    "\n",
    "# ======================= #\n",
    "# 2. Hitung Unique Users dan Items\n",
    "# ======================= #\n",
    "unique_users = pd.unique(df[[\"reviewerID_Movie\", \"reviewerID_Book\"]].values.ravel())\n",
    "unique_items = pd.unique(df[[\"Movie_id\", \"Book_id\"]].values.ravel())\n",
    "\n",
    "num_users = len(unique_users)\n",
    "num_items = len(unique_items)\n",
    "num_interactions = len(df)\n",
    "\n",
    "# ======================= #\n",
    "# 3. Hitung Sparsity\n",
    "# ======================= #\n",
    "possible_interactions = num_users * num_items\n",
    "sparsity = 1 - (num_interactions / possible_interactions)\n",
    "\n",
    "print(f\"🧮 Jumlah User Unik      : {num_users}\")\n",
    "print(f\"🧮 Jumlah Item Unik      : {num_items}\")\n",
    "print(f\"✅ Jumlah Interaksi      : {num_interactions}\")\n",
    "print(f\"📉 Sparsity              : {sparsity:.6f} (atau {sparsity*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487761b",
   "metadata": {},
   "source": [
    "## Fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb3d641",
   "metadata": {},
   "source": [
    "### GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d888a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_14220\\3991646328.py:3: DtypeWarning: Columns (2,3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv(\"Dataset_Books_GEMINI.csv\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(\"Dataset_Books_GEMINI.csv\")\n",
    "df2 = pd.read_csv(\"Dataset_Movies_and_TV_GEMINI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75e8d512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>head_id</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>tail_id</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:genre</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:genre</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>591833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:literaryGenre</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dbo:literaryGenre</td>\n",
       "      <td>cat:Hymnal</td>\n",
       "      <td>591833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MNTGGZBSGA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Hymns</td>\n",
       "      <td>366595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id  rating                                       entity_id  \\\n",
       "0  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "1  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "2  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "3  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "4  MNTGGZBSGA     1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
       "\n",
       "                                          head_id        relation_id  \\\n",
       "0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal          dbo:genre   \n",
       "1  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal          dbo:genre   \n",
       "2  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal  dbo:literaryGenre   \n",
       "3  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal  dbo:literaryGenre   \n",
       "4  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal        dct:subject   \n",
       "\n",
       "      tail_id  reviewerID  \n",
       "0  cat:Hymnal      366595  \n",
       "1  cat:Hymnal      591833  \n",
       "2  cat:Hymnal      366595  \n",
       "3  cat:Hymnal      591833  \n",
       "4   cat:Hymns      366595  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46d503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>head_id</th>\n",
       "      <th>relation_id</th>\n",
       "      <th>tail_id</th>\n",
       "      <th>reviewerID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Christianity</td>\n",
       "      <td>586297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Christianity</td>\n",
       "      <td>226404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Christianity</td>\n",
       "      <td>178001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Children's_Music</td>\n",
       "      <td>586297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>dct:subject</td>\n",
       "      <td>cat:Children's_Music</td>\n",
       "      <td>226404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      item_id  rating                                          entity_id  \\\n",
       "0  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "1  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "2  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "3  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "4  GAYDAMJVGI     1.0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "\n",
       "                                             head_id  relation_id  \\\n",
       "0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dct:subject   \n",
       "1  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dct:subject   \n",
       "2  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dct:subject   \n",
       "3  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dct:subject   \n",
       "4  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...  dct:subject   \n",
       "\n",
       "                tail_id  reviewerID  \n",
       "0      cat:Christianity      586297  \n",
       "1      cat:Christianity      226404  \n",
       "2      cat:Christianity      178001  \n",
       "3  cat:Children's_Music      586297  \n",
       "4  cat:Children's_Music      226404  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7ae8a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_21428\\4214430225.py:17: DtypeWarning: Columns (3,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_books = pd.read_csv(books_path, usecols=[\"item_id\", \"head_id\", \"tail_id\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Movie_id                                         Judul_Film  \\\n",
      "0      GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "1      GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "2      GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "3      GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "4      GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "...           ...                                                ...   \n",
      "55005  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "55006  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "55007  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "55008  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "55009  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "\n",
      "            Relation                                         Judul_Buku  \\\n",
      "0       Christianity                         res:The_Book_of_Revelation   \n",
      "1       Christianity                            res:Celtic_Daily_Prayer   \n",
      "2       Christianity  res:A_Year_With_C._S._Lewis_:_365_Daily_Readin...   \n",
      "3       Christianity  res:The_Jesus_Papers:_Exposing_the_Greatest_Co...   \n",
      "4       Christianity                            res:The_Last_Temptation   \n",
      "...              ...                                                ...   \n",
      "55005  United_States                                    res:The_Prophet   \n",
      "55006  United_States  res:The_Berenstains'_B_Book_(Bright_&_Early_Bo...   \n",
      "55007        English  res:The_Very_Bad_Bunny_(A_Beginner_Book)_(Begi...   \n",
      "55008        English  res:You_Can&rsquo;t_Read_This_Book:_Censorship...   \n",
      "55009      Hollywood             res:I_Heart_Hollywood_(I_Heart_Series)   \n",
      "\n",
      "          Book_id  \n",
      "0      MFQWEMZSGM  \n",
      "1      MVSWENRZME  \n",
      "2      GQZTAYZTGY  \n",
      "3      MRRTKODFGN  \n",
      "4      GVSDMMJWMR  \n",
      "...           ...  \n",
      "55005  MM4DCZJXGI  \n",
      "55006  GRSTOMZSMN  \n",
      "55007  GE4DEYTFGB  \n",
      "55008  GFRTMNLDMV  \n",
      "55009  GRSDEZJXMJ  \n",
      "\n",
      "[55010 rows x 5 columns]\n",
      "\n",
      "✅ Ditemukan 55010 koneksi antar domain berdasarkan relasi yang identik!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# ==========================#\n",
    "# Konfigurasi File & API\n",
    "# ==========================#\n",
    "\n",
    "movies_path = \"Dataset_Movies_and_TV_GEMINI.csv\"\n",
    "books_path = \"Dataset_Books_GEMINI.csv\"\n",
    "\n",
    "# ==========================#\n",
    "# Load dan Bersihkan Data\n",
    "# ==========================#\n",
    "\n",
    "# Load dataset\n",
    "df_movies = pd.read_csv(movies_path, usecols=[\"item_id\", \"head_id\", \"tail_id\"])\n",
    "df_books = pd.read_csv(books_path, usecols=[\"item_id\", \"head_id\", \"tail_id\"])\n",
    "\n",
    "# Hilangkan prefix di tail_id\n",
    "df_movies[\"tail_id\"] = df_movies[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "df_books[\"tail_id\"] = df_books[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "\n",
    "# Hilangkan duplikat\n",
    "df_movies = df_movies.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "df_books = df_books.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "\n",
    "# Cari tail_id yang muncul di kedua domain\n",
    "common_tail_ids = set(df_movies[\"tail_id\"]).intersection(set(df_books[\"tail_id\"]))\n",
    "\n",
    "# Filter berdasarkan relasi yang sama\n",
    "df_movies_filtered = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "df_books_filtered = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# Merge manual berdasarkan relasi yang sama (tail_id)\n",
    "merged_df = pd.merge(df_movies_filtered, df_books_filtered, on=\"tail_id\", suffixes=(\"_movies\", \"_books\"))\n",
    "\n",
    "# Ambil kolom yang diperlukan\n",
    "final_df = merged_df[[\n",
    "    \"item_id_movies\", \"head_id_movies\", \"tail_id\", \"head_id_books\", \"item_id_books\"\n",
    "]].rename(columns={\n",
    "    \"item_id_movies\": \"Movie_id\",\n",
    "    \"head_id_movies\": \"Judul_Film\",\n",
    "    \"tail_id\": \"Relation\",\n",
    "    \"head_id_books\": \"Judul_Buku\",\n",
    "    \"item_id_books\": \"Book_id\"\n",
    "})\n",
    "\n",
    "# ==========================#\n",
    "# Output\n",
    "# ==========================#\n",
    "\n",
    "print(final_df)\n",
    "final_df.to_csv(\"Movies_Books_Manual_Merge_GEMINI.csv\", index=False)\n",
    "print(f\"\\n✅ Ditemukan {len(final_df)} koneksi antar domain berdasarkan relasi yang identik!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8118feff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity (tanpa user_id): 0.8565\n"
     ]
    }
   ],
   "source": [
    "# ==========================#\n",
    "# Hitung Sparsity\n",
    "# ==========================#\n",
    "\n",
    "# Hitung jumlah unik item Movie dan Book dari final_df\n",
    "n_movie_items = final_df[\"Movie_id\"].nunique()\n",
    "n_book_items = final_df[\"Book_id\"].nunique()\n",
    "\n",
    "# Hitung jumlah pasangan aktual (baris dalam final_df)\n",
    "n_actual_pairs = len(final_df)\n",
    "\n",
    "# Hitung sparsity\n",
    "sparsity = 1 - (n_actual_pairs / (n_movie_items * n_book_items))\n",
    "print(f\"Sparsity (tanpa user_id): {sparsity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b54f3d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_id</th>\n",
       "      <th>Judul_Film</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Judul_Buku</th>\n",
       "      <th>Book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:The_Book_of_Revelation</td>\n",
       "      <td>MFQWEMZSGM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:Celtic_Daily_Prayer</td>\n",
       "      <td>MVSWENRZME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:A_Year_With_C._S._Lewis_:_365_Daily_Readin...</td>\n",
       "      <td>GQZTAYZTGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:The_Jesus_Papers:_Exposing_the_Greatest_Co...</td>\n",
       "      <td>MRRTKODFGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:The_Last_Temptation</td>\n",
       "      <td>GVSDMMJWMR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Movie_id                                         Judul_Film  \\\n",
       "0  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "1  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "2  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "3  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "4  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "\n",
       "       Relation                                         Judul_Buku     Book_id  \n",
       "0  Christianity                         res:The_Book_of_Revelation  MFQWEMZSGM  \n",
       "1  Christianity                            res:Celtic_Daily_Prayer  MVSWENRZME  \n",
       "2  Christianity  res:A_Year_With_C._S._Lewis_:_365_Daily_Readin...  GQZTAYZTGY  \n",
       "3  Christianity  res:The_Jesus_Papers:_Exposing_the_Greatest_Co...  MRRTKODFGN  \n",
       "4  Christianity                            res:The_Last_Temptation  GVSDMMJWMR  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b = pd.read_csv(\"Movies_Books_Manual_Merge_GEMINI.csv\")\n",
    "df_b.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde5cff1",
   "metadata": {},
   "source": [
    "### GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4a4cfd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Movie_id                                         Judul_Film  \\\n",
      "0       GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "1       GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "2       GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "3       GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "4       GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "...            ...                                                ...   \n",
      "157324  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "157325  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "157326  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "157327  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "157328  GYZTANJSGY                             res:Permanent_Midnight   \n",
      "\n",
      "             Relation                                         Judul_Buku  \\\n",
      "0        Christianity                        res:Biography_of_C.S._Lewis   \n",
      "1        Christianity  res:Jesus_Rediscovered_(Fontana_Religious_Series)   \n",
      "2        Christianity  res:The_pilgrim's_regress:_An_allegorical_apol...   \n",
      "3        Christianity                              res:Jesus_of_Nazareth   \n",
      "4        Christianity                               res:Strength_to_Love   \n",
      "...               ...                                                ...   \n",
      "157324  United_States                           res:Hitler's_Last_Gamble   \n",
      "157325  United_States                            res:The_Art_of_Dreaming   \n",
      "157326  United_States  res:La_Storia:_Five_Centuries_of_the_Italian_A...   \n",
      "157327  United_States                           res:The_Poisonwood_Bible   \n",
      "157328  United_States                                      res:Up_Island   \n",
      "\n",
      "           Book_id  \n",
      "0       GJQTGODBGR  \n",
      "1       HBSDGYTCME  \n",
      "2       GU3TGN3DGZ  \n",
      "3       MYZGMYZZHE  \n",
      "4       HA4WMMDGMQ  \n",
      "...            ...  \n",
      "157324  MJSDGZLGGV  \n",
      "157325  G4ZWGMJUGA  \n",
      "157326  GU3DKNZXHA  \n",
      "157327  MVRWMNJWGM  \n",
      "157328  MRSTANBTME  \n",
      "\n",
      "[157329 rows x 5 columns]\n",
      "\n",
      "✅ Ditemukan 157329 koneksi antar domain berdasarkan relasi yang identik!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# ==========================#\n",
    "# Konfigurasi File & API\n",
    "# ==========================#\n",
    "\n",
    "movies_path = \"Dataset_Movies_and_TV.csv\"\n",
    "books_path = \"Dataset_Books.csv\"\n",
    "\n",
    "# ==========================#\n",
    "# Load dan Bersihkan Data\n",
    "# ==========================#\n",
    "\n",
    "# Load dataset\n",
    "df_movies = pd.read_csv(movies_path, usecols=[\"item_id\", \"head_id\", \"tail_id\"])\n",
    "df_books = pd.read_csv(books_path, usecols=[\"item_id\", \"head_id\", \"tail_id\"])\n",
    "\n",
    "# Hilangkan prefix di tail_id\n",
    "df_movies[\"tail_id\"] = df_movies[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "df_books[\"tail_id\"] = df_books[\"tail_id\"].str.replace(r\"(res:|cat:)\", \"\", regex=True).str.strip()\n",
    "\n",
    "# Hilangkan duplikat\n",
    "df_movies = df_movies.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "df_books = df_books.drop_duplicates(subset=[\"tail_id\", \"item_id\", \"head_id\"])\n",
    "\n",
    "# Cari tail_id yang muncul di kedua domain\n",
    "common_tail_ids = set(df_movies[\"tail_id\"]).intersection(set(df_books[\"tail_id\"]))\n",
    "\n",
    "# Filter berdasarkan relasi yang sama\n",
    "df_movies_filtered = df_movies[df_movies[\"tail_id\"].isin(common_tail_ids)]\n",
    "df_books_filtered = df_books[df_books[\"tail_id\"].isin(common_tail_ids)]\n",
    "\n",
    "# Merge manual berdasarkan relasi yang sama (tail_id)\n",
    "merged_df = pd.merge(df_movies_filtered, df_books_filtered, on=\"tail_id\", suffixes=(\"_movies\", \"_books\"))\n",
    "\n",
    "# Ambil kolom yang diperlukan\n",
    "final_df = merged_df[[\n",
    "    \"item_id_movies\", \"head_id_movies\", \"tail_id\", \"head_id_books\", \"item_id_books\"\n",
    "]].rename(columns={\n",
    "    \"item_id_movies\": \"Movie_id\",\n",
    "    \"head_id_movies\": \"Judul_Film\",\n",
    "    \"tail_id\": \"Relation\",\n",
    "    \"head_id_books\": \"Judul_Buku\",\n",
    "    \"item_id_books\": \"Book_id\"\n",
    "})\n",
    "\n",
    "# ==========================#\n",
    "# Output\n",
    "# ==========================#\n",
    "\n",
    "print(final_df)\n",
    "final_df.to_csv(\"Movies_Books_Manual_Merge_GPT.csv\", index=False)\n",
    "print(f\"\\n✅ Ditemukan {len(final_df)} koneksi antar domain berdasarkan relasi yang identik!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f14a0fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity (tanpa user_id): 0.9346\n"
     ]
    }
   ],
   "source": [
    "# ==========================#\n",
    "# Hitung Sparsity\n",
    "# ==========================#\n",
    "\n",
    "# Hitung jumlah unik item Movie dan Book dari final_df\n",
    "n_movie_items = final_df[\"Movie_id\"].nunique()\n",
    "n_book_items = final_df[\"Book_id\"].nunique()\n",
    "\n",
    "# Hitung jumlah pasangan aktual (baris dalam final_df)\n",
    "n_actual_pairs = len(final_df)\n",
    "\n",
    "# Hitung sparsity\n",
    "sparsity = 1 - (n_actual_pairs / (n_movie_items * n_book_items))\n",
    "print(f\"Sparsity (tanpa user_id): {sparsity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9414b852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie_id</th>\n",
       "      <th>Judul_Film</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Judul_Buku</th>\n",
       "      <th>Book_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:Biography_of_C.S._Lewis</td>\n",
       "      <td>GJQTGODBGR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:Jesus_Rediscovered_(Fontana_Religious_Series)</td>\n",
       "      <td>HBSDGYTCME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:The_pilgrim's_regress:_An_allegorical_apol...</td>\n",
       "      <td>GU3TGN3DGZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:Jesus_of_Nazareth</td>\n",
       "      <td>MYZGMYZZHE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GAYDAMJVGI</td>\n",
       "      <td>res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...</td>\n",
       "      <td>Christianity</td>\n",
       "      <td>res:Strength_to_Love</td>\n",
       "      <td>HA4WMMDGMQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Movie_id                                         Judul_Film  \\\n",
       "0  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "1  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "2  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "3  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "4  GAYDAMJVGI  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
       "\n",
       "       Relation                                         Judul_Buku     Book_id  \n",
       "0  Christianity                        res:Biography_of_C.S._Lewis  GJQTGODBGR  \n",
       "1  Christianity  res:Jesus_Rediscovered_(Fontana_Religious_Series)  HBSDGYTCME  \n",
       "2  Christianity  res:The_pilgrim's_regress:_An_allegorical_apol...  GU3TGN3DGZ  \n",
       "3  Christianity                              res:Jesus_of_Nazareth  MYZGMYZZHE  \n",
       "4  Christianity                               res:Strength_to_Love  HA4WMMDGMQ  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b2 = pd.read_csv(\"Movies_Books_Manual_Merge_GPT.csv\")\n",
    "df_b2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39bad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df.to_csv(\"Movies_Books_LLM_KG_GPT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5be7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ambil tail_id yang sama\n",
    "# common_tail_ids = set(df_books['tail_id']).intersection(df_movies['tail_id'])\n",
    "# df_books = df_books[df_books['tail_id'].isin(common_tail_ids)].copy()\n",
    "# df_movies = df_movies[df_movies['tail_id'].isin(common_tail_ids)].copy()\n",
    "\n",
    "# # Potong df_movies jadi batch kecil (misalnya 100 bagian)\n",
    "# chunks = np.array_split(df_movies, 1000)\n",
    "\n",
    "# result_list = []\n",
    "\n",
    "# for i, chunk in enumerate(chunks):\n",
    "#     merged = pd.merge(\n",
    "#         chunk,\n",
    "#         df_books,\n",
    "#         on=\"tail_id\",\n",
    "#         how=\"inner\",\n",
    "#         suffixes=(\"_movies\", \"_books\")\n",
    "#     )\n",
    "\n",
    "#     merged = merged[\n",
    "#         [\"item_id_movies\", \"head_id_movies\", \"tail_id\", \"head_id_books\", \"item_id_books\", \"reviewerID_books\"]\n",
    "#     ].rename(columns={\n",
    "#         \"item_id_movies\": \"Movie_ID\",\n",
    "#         \"head_id_movies\": \"Judul_Film\",\n",
    "#         \"tail_id\": \"Relation\",\n",
    "#         \"head_id_books\": \"Judul_Buku\",\n",
    "#         \"item_id_books\": \"Book_ID\",\n",
    "#         \"reviewerID_books\": \"User_ID\"\n",
    "#     })\n",
    "\n",
    "#     # Simpan hasil per batch\n",
    "#     merged.to_csv(f\"merged_batch_{i+1}.csv\", index=False)\n",
    "#     print(f\"✅ Batch {i+1}/{len(chunks)} disimpan, {len(merged)} baris\")\n",
    "\n",
    "# # Gabungkan semua batch\n",
    "# final_df = pd.concat(result_list, ignore_index=True)\n",
    "\n",
    "# # (Optional) Hapus duplikat jika diperlukan\n",
    "# final_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# # Simpan ke CSV\n",
    "# final_df.to_csv(\"Cross-domain-GPT.csv\", index=False)\n",
    "# print(\"✅ File berhasil disimpan dengan total:\", len(final_df), \"baris\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
