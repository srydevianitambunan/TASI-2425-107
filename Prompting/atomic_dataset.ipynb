{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         user_id        item_id     timestamp\n",
      "0  user_id:token  item_id:token  rating:float\n",
      "1         586297     GAYDAMJVGI           NaN\n",
      "2         226404     GAYDAMJVGI           NaN\n",
      "3         178001     GAYDAMJVGI           NaN\n",
      "4         388575     GAYDAMJVGI           NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_46280\\70701515.py:7: DtypeWarning: Columns (0,2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_inter = pd.read_csv(file_path, sep='\\t', header=None, names=[\"user_id\", \"item_id\", \"timestamp\"])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Buka file .inter\n",
    "file_path = 'Amazon_Movies_and_TV.inter'\n",
    "\n",
    "# Membaca file .inter sebagai dataframe\n",
    "df_inter = pd.read_csv(file_path, sep='\\t', header=None, names=[\"user_id\", \"item_id\", \"timestamp\"])\n",
    "\n",
    "# Lihat beberapa baris awal\n",
    "print(df_inter.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id:token  rating:float  \\\n",
      "0    GAYDAMJVGI           1.0   \n",
      "1    GAYDAMJVGI           1.0   \n",
      "2    GAYDAMJVGI           1.0   \n",
      "3    GAYDAMJVGI           1.0   \n",
      "4    GAYDAMJVGI           1.0   \n",
      "\n",
      "                                     entity_id:token  \\\n",
      "0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "1  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "2  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "3  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "4  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...   \n",
      "\n",
      "                                       head_id:token relation_id:token  \\\n",
      "0  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...      dbo:director   \n",
      "1  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...      dbo:director   \n",
      "2  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...      dbo:director   \n",
      "3  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...      dbo:director   \n",
      "4  res:Steve_Green:_Hide_'em_in_Your_Heart:_13_Bi...      dbo:director   \n",
      "\n",
      "          tail_id:token  reviewerID:token  \n",
      "0  res:Michael_Merriman            586297  \n",
      "1  res:Michael_Merriman            226404  \n",
      "2  res:Michael_Merriman            178001  \n",
      "3  res:Michael_Merriman            388575  \n",
      "4  res:Michael_Merriman            515515  \n"
     ]
    }
   ],
   "source": [
    "df_movies =  pd.read_csv('Dataset_Movies&TV.csv')\n",
    "\n",
    "print(df_movies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "772\n"
     ]
    }
   ],
   "source": [
    "print(df_movies['item_id:token'].nunique())  # Menghitung jumlah item unik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id:token  rating:float                                 entity_id:token  \\\n",
      "0    MNTGGZBSGA           1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
      "1    MNTGGZBSGA           1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
      "2    MNTGGZBSGA           1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
      "3    MNTGGZBSGA           1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
      "4    MNTGGZBSGA           1.0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal   \n",
      "\n",
      "                                    head_id:token relation_id:token  \\\n",
      "0  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal       dct:subject   \n",
      "1  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal       dct:subject   \n",
      "2  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal        dbo:author   \n",
      "3  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal        dbo:author   \n",
      "4  res:Heavenly_Highway_Hymns:_Shaped-Note_Hymnal      dbo:composer   \n",
      "\n",
      "     tail_id:token  reviewerID:token  \n",
      "0       cat:Hymnal            366595  \n",
      "1       cat:Hymnal            591833  \n",
      "2    res:John_Shaw            366595  \n",
      "3    res:John_Shaw            591833  \n",
      "4  res:B._F._White            366595  \n"
     ]
    }
   ],
   "source": [
    "df_books = pd.read_csv('Dataset_Book.csv')\n",
    "\n",
    "print(df_books.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4271\n"
     ]
    }
   ],
   "source": [
    "print(df_books['item_id:token'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split into:\n",
      "- Amazon_Movies_and_TV.inter\n",
      "- Amazon_Movies_and_TV.kg\n",
      "- Amazon_Movies_and_TV.link\n"
     ]
    }
   ],
   "source": [
    "# Define input and output files\n",
    "input_file = \"Dataset_Movies&TV.csv\"\n",
    "inter_file = \"Amazon_Movies_and_TV.inter\"\n",
    "kg_file = \"Amazon_Movies_and_TV.kg\"\n",
    "link_file = \"Amazon_Movies_and_TV.link\"\n",
    "\n",
    "# Read in chunks to handle large files\n",
    "chunk_size = 100000  \n",
    "\n",
    "# Create empty files with headers\n",
    "pd.DataFrame(columns=[\"user_id:token\", \"item_id:token\", \"rating:float\"]).to_csv(inter_file, index=False, sep=\"\\t\")\n",
    "pd.DataFrame(columns=[\"head_id:token\", \"relation_id:token\", \"tail_id:token\"]).to_csv(kg_file, index=False, sep=\"\\t\")\n",
    "pd.DataFrame(columns=[\"item_id:token\", \"entity_id:token\"]).to_csv(link_file, index=False, sep=\"\\t\")\n",
    "\n",
    "# Process file in chunks\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False):\n",
    "    # Extract relevant data\n",
    "    inter_data = chunk[[\"reviewerID:token\", \"item_id:token\"]].rename(columns={\"reviewerID:token\": \"user_id:token\"})\n",
    "    kg_data = chunk[[\"head_id:token\", \"relation_id:token\", \"tail_id:token\"]]\n",
    "    link_data = chunk[[\"item_id:token\", \"entity_id:token\"]]\n",
    "\n",
    "    # Append data to respective files (tab-separated)\n",
    "    inter_data.to_csv(inter_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "    kg_data.to_csv(kg_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "    link_data.to_csv(link_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"✅ Data successfully split into:\")\n",
    "print(\"- Amazon_Movies_and_TV.inter\")\n",
    "print(\"- Amazon_Movies_and_TV.kg\")\n",
    "print(\"- Amazon_Movies_and_TV.link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split into:\n",
      "- Amazon_Books.inter\n",
      "- Amazon_Books.kg\n",
      "- Amazon_Books.link\n"
     ]
    }
   ],
   "source": [
    "# Define input and output files\n",
    "input_file2 = \"Dataset_Book.csv\"\n",
    "inter_file = \"Amazon_Books.inter\"\n",
    "kg_file = \"Amazon_Books.kg\"\n",
    "link_file = \"Amazon_Books.link\"\n",
    "\n",
    "# Read in chunks to handle large files\n",
    "chunk_size = 100000  \n",
    "\n",
    "# Create empty files with headers\n",
    "pd.DataFrame(columns=[\"user_id:token\", \"item_id:token\", \"rating:float\"]).to_csv(inter_file, index=False, sep=\"\\t\")\n",
    "pd.DataFrame(columns=[\"head_id:token\", \"relation_id:token\", \"tail_id:token\"]).to_csv(kg_file, index=False, sep=\"\\t\")\n",
    "pd.DataFrame(columns=[\"item_id:token\", \"entity_id:token\"]).to_csv(link_file, index=False, sep=\"\\t\")\n",
    "\n",
    "# Process file in chunks\n",
    "for chunk in pd.read_csv(input_file2, chunksize=chunk_size, low_memory=False):\n",
    "    # Extract relevant data\n",
    "    inter_data = chunk[[\"reviewerID:token\", \"item_id:token\"]].rename(columns={\"reviewerID:token\": \"user_id:token\"})\n",
    "    kg_data = chunk[[\"head_id:token\", \"relation_id:token\", \"tail_id:token\"]]\n",
    "    link_data = chunk[[\"item_id:token\", \"entity_id:token\"]]\n",
    "\n",
    "    # Append data to respective files (tab-separated)\n",
    "    inter_data.to_csv(inter_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "    kg_data.to_csv(kg_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "    link_data.to_csv(link_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"✅ Data successfully split into:\")\n",
    "print(\"- Amazon_Books.inter\")\n",
    "print(\"- Amazon_Books.kg\")\n",
    "print(\"- Amazon_Books.link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split into:\n",
      "- Amazon_Movies_and_TV.item\n"
     ]
    }
   ],
   "source": [
    "# Define input and output files\n",
    "input_file = \"Dataset_Movies&TV.csv\"\n",
    "item_file = \"Amazon_Movies_and_TV.item\"\n",
    "\n",
    "# Read in chunks to handle large files\n",
    "chunk_size = 100000  \n",
    "\n",
    "# Create empty files with headers\n",
    "pd.DataFrame(columns=[\"item_id:token\"]).to_csv(item_file, index=False, sep=\"\\t\")\n",
    "\n",
    "# Process file in chunks\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False):\n",
    "    # Extract relevant data\n",
    "    item_data = chunk[[\"item_id:token\"]]\n",
    "\n",
    "    # Append data to respective files (tab-separated)\n",
    "    item_data.to_csv(item_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"✅ Data successfully split into:\")\n",
    "print(\"- Amazon_Movies_and_TV.item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data successfully split into:\n",
      "- Amazon_Books.item\n"
     ]
    }
   ],
   "source": [
    "# Define input and output files\n",
    "input_file2 = \"Dataset_Book.csv\"\n",
    "item_file = \"Amazon_Books.item\"\n",
    "\n",
    "# Read in chunks to handle large files\n",
    "chunk_size = 100000  \n",
    "\n",
    "# Create empty files with headers\n",
    "pd.DataFrame(columns=[\"item_id:token\"]).to_csv(item_file, index=False, sep=\"\\t\")\n",
    "\n",
    "# Process file in chunks\n",
    "for chunk in pd.read_csv(input_file2, chunksize=chunk_size, low_memory=False):\n",
    "    # Extract relevant data\n",
    "    item_data = chunk[[\"item_id:token\"]]\n",
    "\n",
    "    # Append data to respective files (tab-separated)\n",
    "    item_data.to_csv(item_file, mode=\"a\", header=False, index=False, sep=\"\\t\")\n",
    "\n",
    "print(\"✅ Data successfully split into:\")\n",
    "print(\"- Amazon_Books.item\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris duplikat: 845410\n",
      "    user_id:token item_id:token  rating:float\n",
      "6          586297    GAYDAMJVGI           NaN\n",
      "7          226404    GAYDAMJVGI           NaN\n",
      "8          178001    GAYDAMJVGI           NaN\n",
      "9          388575    GAYDAMJVGI           NaN\n",
      "10         515515    GAYDAMJVGI           NaN\n"
     ]
    }
   ],
   "source": [
    "# Mengecek duplikat\n",
    "# Baca file\n",
    "df = pd.read_csv('Amazon_Movies_and_TV.inter', sep='\\t')\n",
    "\n",
    "# Cek baris yang duplikat (semua kolom sama persis)\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "# Tampilkan baris duplikat (jika ada)\n",
    "print(f\"Jumlah baris duplikat: {len(duplicates)}\")\n",
    "print(duplicates.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
