{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_57852\\4135744597.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_empty_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_57852\\4135744597.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_comma_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris dengan nilai [] di setiap kolom:\n",
      " description    119328\n",
      "title               0\n",
      "asin                0\n",
      "dtype: int64\n",
      "Jumlah baris dengan nilai [, ] di setiap kolom:\n",
      " description    0\n",
      "title          0\n",
      "asin           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ukuran chunk\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Buka ulang iterator setiap kali ingin membacanya\n",
    "df1_iter = pd.read_json('filtered_meta_Books.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "for df1_chunk in df1_iter:\n",
    "    # Pastikan kolom hanya berisi list sebelum melakukan applymap\n",
    "    count_empty_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
    "    count_comma_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n",
    "\n",
    "    print(\"Jumlah baris dengan nilai [] di setiap kolom:\\n\", count_empty_list)\n",
    "    print(\"Jumlah baris dengan nilai [, ] di setiap kolom:\\n\", count_comma_list)\n",
    "    \n",
    "    # Hentikan setelah satu chunk (hapus jika ingin menghitung semua)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris dengan nilai [] di seluruh dataset: 0\n"
     ]
    }
   ],
   "source": [
    "total_empty_list = 0\n",
    "\n",
    "for df1_chunk in df1_iter:\n",
    "    total_empty_list += df1_chunk['description'].apply(lambda x: x == []).sum()\n",
    "\n",
    "print(f\"Total baris dengan nilai [] di seluruh dataset: {total_empty_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File bersih telah disimpan sebagai 'filtered_meta_Books_cleaned.json'\n"
     ]
    }
   ],
   "source": [
    "# Membaca file JSON dalam mode chunk\n",
    "chunk_size = 1000000  # Sesuaikan dengan kapasitas memori\n",
    "df1_iter = pd.read_json('filtered_meta_Books.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "# File output untuk menyimpan hasil\n",
    "output_file = 'filtered_meta_Books_cleaned.json'\n",
    "\n",
    "# Membuka file untuk menulis hasil bersih\n",
    "with open(output_file, 'w') as f_out:\n",
    "    for df1_chunk in df1_iter:\n",
    "        # Filter baris yang tidak kosong di kolom 'description'\n",
    "        df1_filtered = df1_chunk[df1_chunk['description'].apply(lambda x: x != [])]\n",
    "        \n",
    "        # Simpan ke file JSON (append untuk tiap chunk)\n",
    "        df1_filtered.to_json(f_out, orient='records', lines=True)\n",
    "\n",
    "print(f\"File bersih telah disimpan sebagai '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_22616\\2466272136.py:11: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_empty_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_22616\\2466272136.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_comma_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris dengan nilai [] di setiap kolom:\n",
      " reviewerID    0\n",
      "asin          0\n",
      "overall       0\n",
      "reviewTime    0\n",
      "dtype: int64\n",
      "Jumlah baris dengan nilai [, ] di setiap kolom:\n",
      " reviewerID    0\n",
      "asin          0\n",
      "overall       0\n",
      "reviewTime    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ukuran chunk\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Buka ulang iterator setiap kali ingin membacanya\n",
    "df2_iter = pd.read_json('filtered_Books.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "\n",
    "\n",
    "for df2_chunk in df2_iter:\n",
    "    # Pastikan kolom hanya berisi list sebelum melakukan applymap\n",
    "    count_empty_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
    "    count_comma_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n",
    "\n",
    "    print(\"Jumlah baris dengan nilai [] di setiap kolom:\\n\", count_empty_list)\n",
    "    print(\"Jumlah baris dengan nilai [, ] di setiap kolom:\\n\", count_comma_list)\n",
    "    \n",
    "    # Hentikan setelah satu chunk (hapus jika ingin menghitung semua)\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>title</th>\n",
       "      <th>asin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000000</th>\n",
       "      <td>[, Meda Ryan was born in West Cork and educate...</td>\n",
       "      <td>Tom Barry: IRA Freedom Fighter</td>\n",
       "      <td>1856354806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000001</th>\n",
       "      <td>[, ]</td>\n",
       "      <td>Sarsfield &amp;amp; the Jacobites</td>\n",
       "      <td>1856354083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000002</th>\n",
       "      <td>[Carla Blake was born in South Africa but has ...</td>\n",
       "      <td>Irish Cookbook</td>\n",
       "      <td>1856355047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000003</th>\n",
       "      <td>[&lt;i&gt;The stories that appear are painful, detai...</td>\n",
       "      <td>Melancholy Madness: A Coroner's Casebook</td>\n",
       "      <td>1856354245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000004</th>\n",
       "      <td>[, ]</td>\n",
       "      <td>The Book of Irish Limericks</td>\n",
       "      <td>1856352617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               description  \\\n",
       "2000000  [, Meda Ryan was born in West Cork and educate...   \n",
       "2000001                                               [, ]   \n",
       "2000002  [Carla Blake was born in South Africa but has ...   \n",
       "2000003  [<i>The stories that appear are painful, detai...   \n",
       "2000004                                               [, ]   \n",
       "\n",
       "                                            title        asin  \n",
       "2000000            Tom Barry: IRA Freedom Fighter  1856354806  \n",
       "2000001             Sarsfield &amp; the Jacobites  1856354083  \n",
       "2000002                            Irish Cookbook  1856355047  \n",
       "2000003  Melancholy Madness: A Coroner's Casebook  1856354245  \n",
       "2000004               The Book of Irish Limericks  1856352617  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1C6M8LCIX4M6M</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>5</td>\n",
       "      <td>08 12, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1REUF3A1YCPHM</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>5</td>\n",
       "      <td>03 30, 2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1YRBRK2XM5D5</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>5</td>\n",
       "      <td>04 4, 2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1V8ZR5P78P4ZU</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>5</td>\n",
       "      <td>02 21, 2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2ZB06582NXCIV</td>\n",
       "      <td>0001713353</td>\n",
       "      <td>5</td>\n",
       "      <td>10 3, 2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin  overall   reviewTime\n",
       "0  A1C6M8LCIX4M6M  0001713353        5  08 12, 2005\n",
       "1  A1REUF3A1YCPHM  0001713353        5  03 30, 2005\n",
       "2   A1YRBRK2XM5D5  0001713353        5   04 4, 2004\n",
       "3  A1V8ZR5P78P4ZU  0001713353        5  02 21, 2004\n",
       "4  A2ZB06582NXCIV  0001713353        5   10 3, 2016"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil merge telah disimpan sebagai 'merge_Books.csv'\n"
     ]
    }
   ],
   "source": [
    "# Membaca file JSON dalam mode chunk untuk efisiensi memori\n",
    "chunk_size = 1000000  \n",
    "df1_iter = pd.read_json('filtered_meta_Books_cleaned.json', lines=True, chunksize=chunk_size)\n",
    "df2_iter = pd.read_json('filtered_Books.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "# File output untuk hasil merge\n",
    "output_file = 'merge_Books.csv'\n",
    "\n",
    "# Membuka file CSV untuk menulis hasil merge\n",
    "with open(output_file, 'w') as f_out:\n",
    "    header_written = False  # Pastikan header hanya ditulis sekali\n",
    "    \n",
    "    for df1_chunk, df2_chunk in zip(df1_iter, df2_iter):\n",
    "        # Menggabungkan berdasarkan 'asin'\n",
    "        merge_Books = pd.merge(df1_chunk, df2_chunk, on='asin', how='inner')\n",
    "\n",
    "        # Menyimpan ke file CSV (append untuk tiap chunk)\n",
    "        merge_Books.to_csv(f_out, mode='a', index=False, header=not header_written)\n",
    "        header_written = True  # Setelah iterasi pertama, header tidak ditulis lagi\n",
    "\n",
    "print(f\"Hasil merge telah disimpan sebagai '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  ['This is a collection of classic gospel hymns...   \n",
      "1  ['This is a collection of classic gospel hymns...   \n",
      "2  ['This is a collection of classic gospel hymns...   \n",
      "3  ['This is a collection of classic gospel hymns...   \n",
      "4  ['This is a collection of classic gospel hymns...   \n",
      "\n",
      "                                        title        asin      reviewerID  \\\n",
      "0  Heavenly Highway Hymns: Shaped-Note Hymnal  0000013765  A3BED5QFJWK88M   \n",
      "1  Heavenly Highway Hymns: Shaped-Note Hymnal  0000013765   AYEDW3BFK53XK   \n",
      "2  Heavenly Highway Hymns: Shaped-Note Hymnal  0000013765  A2EIPZNHAEXZHJ   \n",
      "3  Heavenly Highway Hymns: Shaped-Note Hymnal  0000013765  A37W6POFWIVG13   \n",
      "4  Heavenly Highway Hymns: Shaped-Note Hymnal  0000013765  A2SUAM1J3GNN3B   \n",
      "\n",
      "   overall   reviewTime  \n",
      "0        4  10 16, 2012  \n",
      "1        5   01 2, 2012  \n",
      "2        4  12 28, 2011  \n",
      "3        5  09 16, 2011  \n",
      "4        5  09 13, 2009  \n"
     ]
    }
   ],
   "source": [
    "# Membaca file CSV hasil merge\n",
    "merge_Books = pd.read_csv('merge_Books.csv')\n",
    "\n",
    "# Menampilkan 5 baris teratas\n",
    "print(merge_Books.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 752034\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah baris\n",
    "num_rows = merge_Books.shape[0]\n",
    "print(\"Jumlah baris:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import base64\n",
    "\n",
    "# Tokenizing 'reviewerID'\n",
    "label_encoder_reviewer = LabelEncoder()\n",
    "merge_Books['reviewerID'] = label_encoder_reviewer.fit_transform(merge_Books['reviewerID'])\n",
    "\n",
    "# Fungsi untuk mengubah asin menjadi format yang mirip dengan sebelumnya\n",
    "def encode_asin(asin):\n",
    "    encoded = base64.b32encode(asin.encode()).decode()[:10]  # Base32 encoding dengan 10 karakter\n",
    "    return encoded\n",
    "\n",
    "# Terapkan encoding ke kolom 'asin'\n",
    "merge_Books['asin'] = merge_Books['asin'].astype(str).apply(encode_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          description  \\\n",
      "0   ['This is a collection of classic gospel hymns...   \n",
      "1   ['This is a collection of classic gospel hymns...   \n",
      "2   ['This is a collection of classic gospel hymns...   \n",
      "3   ['This is a collection of classic gospel hymns...   \n",
      "4   ['This is a collection of classic gospel hymns...   \n",
      "..                                                ...   \n",
      "95  ['William Shakespeare is widely regarded as th...   \n",
      "96  ['William Shakespeare is widely regarded as th...   \n",
      "97  ['William Shakespeare is widely regarded as th...   \n",
      "98  ['William Shakespeare is widely regarded as th...   \n",
      "99  ['William Shakespeare is widely regarded as th...   \n",
      "\n",
      "                                                title        asin  reviewerID  \\\n",
      "0          Heavenly Highway Hymns: Shaped-Note Hymnal  MNTGGZBSGA      366595   \n",
      "1          Heavenly Highway Hymns: Shaped-Note Hymnal  MNTGGZBSGA      591833   \n",
      "2          Heavenly Highway Hymns: Shaped-Note Hymnal  MNTGGZBSGA      222211   \n",
      "3          Heavenly Highway Hymns: Shaped-Note Hymnal  MNTGGZBSGA      351171   \n",
      "4          Heavenly Highway Hymns: Shaped-Note Hymnal  MNTGGZBSGA      284978   \n",
      "..                                                ...         ...         ...   \n",
      "95  Love's Labour's Lost: Performed by Derek Jacob...  MM4WMMDGHA      462098   \n",
      "96  Love's Labour's Lost: Performed by Derek Jacob...  MM4WMMDGHA      189219   \n",
      "97  Love's Labour's Lost: Performed by Derek Jacob...  MM4WMMDGHA      140728   \n",
      "98  Love's Labour's Lost: Performed by Derek Jacob...  MM4WMMDGHA      510330   \n",
      "99  Love's Labour's Lost: Performed by Derek Jacob...  MM4WMMDGHA       41845   \n",
      "\n",
      "    overall   reviewTime  \n",
      "0         4  10 16, 2012  \n",
      "1         5   01 2, 2012  \n",
      "2         4  12 28, 2011  \n",
      "3         5  09 16, 2011  \n",
      "4         5  09 13, 2009  \n",
      "..      ...          ...  \n",
      "95        5  03 13, 2018  \n",
      "96        5  03 12, 2018  \n",
      "97        4  03 11, 2018  \n",
      "98        5  03 11, 2018  \n",
      "99        3  03 10, 2018  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan hasil 5 baris pertama\n",
    "print(merge_Books.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil tokenization ke file baru\n",
    "merge_Books.to_csv('merge_Books_tokenized(2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 5, Jumlah reviewer unik: 3611\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('merge_Books_tokenized(2).csv')\n",
    "\n",
    "# Pilih 2.000 item pertama berdasarkan 'asin' (pastikan unik)\n",
    "selected_asins = df['asin'].drop_duplicates().head(5)\n",
    "\n",
    "# Filter dataset untuk hanya menyertakan review dari 2.000 item tersebut\n",
    "filtered_df = df[df['asin'].isin(selected_asins)]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "filtered_df.to_csv('Dataset_Books_for_analysis(2).csv', index=False)\n",
    "\n",
    "# Menampilkan jumlah unique item dan reviewer\n",
    "num_items = filtered_df['asin'].nunique()\n",
    "num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 5, Jumlah reviewer unik: 100\n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# df = pd.read_csv('merge_Books_tokenized(2).csv')\n",
    "\n",
    "# # Menghitung jumlah unique reviewerID untuk setiap ASIN\n",
    "# reviewer_counts = df.groupby('asin')['reviewerID'].nunique().reset_index()\n",
    "\n",
    "# # Filter hanya ASIN yang memiliki maksimal 20 reviewer\n",
    "# filtered_asins = reviewer_counts[reviewer_counts['reviewerID'] <= 20]\n",
    "\n",
    "# # Ambil 5 ASIN dengan jumlah reviewer terbanyak dalam batasan ini\n",
    "# top_5_asins = filtered_asins.sort_values(by='reviewerID', ascending=False).head(5)['asin']\n",
    "\n",
    "# # Filter dataset berdasarkan 5 ASIN tersebut\n",
    "# filtered_df = df[df['asin'].isin(top_5_asins)]\n",
    "\n",
    "# # Simpan hasil ke dalam file baru\n",
    "# filtered_df.to_csv('Dataset_Books_for_analysis(2).csv', index=False)\n",
    "\n",
    "# # Menampilkan jumlah unique item dan reviewer\n",
    "# num_items = filtered_df['asin'].nunique()\n",
    "# num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "# print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dengan atribut terpilih telah disimpan sebagai 'Attributes_Books.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "merge_Books = pd.read_csv('merge_Books_tokenized(2).csv')\n",
    "\n",
    "# Pilih atribut tertentu (sesuaikan dengan kebutuhan)\n",
    "selected_columns = ['reviewerID', 'asin', 'title']\n",
    "filtered_df = merge_Books[selected_columns]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "output_file = 'Attributes_Books.csv'\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File dengan atribut terpilih telah disimpan sebagai '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File berhasil dikonversi: Attributes_Books.json\n"
     ]
    }
   ],
   "source": [
    "# Path untuk file input dan output\n",
    "csv_path = 'Attributes_Books.csv'  # Sesuaikan dengan lokasi file\n",
    "json_path = 'Attributes_Books.json'  # Path output JSONL\n",
    "\n",
    "# Membaca CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Menyimpan sebagai JSONL (JSON Lines)\n",
    "df.to_json(json_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"✅ File berhasil dikonversi: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File JSON dalam bentuk array telah disimpan ke Attributes_Books_array.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path file JSON (sesuaikan dengan lokasi file di lokal)\n",
    "json_path = \"Attributes_Books.json\"  # File input JSONL\n",
    "output_json_path = \"Attributes_Books_array.json\"  # Output JSON array\n",
    "\n",
    "# Cek apakah file JSONL ada\n",
    "if not os.path.exists(json_path):\n",
    "    print(f\"❌ File tidak ditemukan: {json_path}\")\n",
    "else:\n",
    "    json_array = []\n",
    "\n",
    "    # Membaca file JSONL dan mengonversi ke format array\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                json_array.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Error decoding JSON: {e}\")\n",
    "\n",
    "    # Menyimpan hasil dalam bentuk array ke file baru\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_array, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ File JSON dalam bentuk array telah disimpan ke {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 1, Jumlah reviewer unik: 5\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('merge_Books_tokenized(2).csv')\n",
    "\n",
    "# Pilih 2.000 item pertama berdasarkan 'asin' (pastikan unik)\n",
    "selected_asins = df['reviewerID'].drop_duplicates().head(5)\n",
    "\n",
    "# Filter dataset untuk hanya menyertakan review dari 2.000 item tersebut\n",
    "filtered_df = df[df['reviewerID'].isin(selected_asins)]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "filtered_df.to_csv('Dataset_Books_for_analysis_reviewerID.csv', index=False)\n",
    "\n",
    "# Menampilkan jumlah unique item dan reviewer\n",
    "num_items = filtered_df['asin'].nunique()\n",
    "num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dengan atribut terpilih telah disimpan sebagai 'selected_attributes_Books_reviewerID.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_analysis = pd.read_csv('Dataset_Books_for_analysis_reviewerID.csv')\n",
    "\n",
    "# Pilih atribut tertentu (sesuaikan dengan kebutuhan)\n",
    "selected_columns = ['reviewerID', 'asin', 'title']\n",
    "filtered_df = df_analysis[selected_columns]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "output_file = 'selected_attributes_Books_reviewerID.csv'\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File dengan atribut terpilih telah disimpan sebagai '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File berhasil dikonversi: selected_attributes_Books_reviewerID.json\n"
     ]
    }
   ],
   "source": [
    "# Path untuk file input dan output\n",
    "csv_path = 'selected_attributes_Books_reviewerID.csv'  # Sesuaikan dengan lokasi file\n",
    "json_path = 'selected_attributes_Books_reviewerID.json'  # Path output JSONL\n",
    "\n",
    "# Membaca CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Menyimpan sebagai JSONL (JSON Lines)\n",
    "df.to_json(json_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"✅ File berhasil dikonversi: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File JSON dalam bentuk array telah disimpan ke selected_attributes_Books_array_reviewerID.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path file JSON (sesuaikan dengan lokasi file di lokal)\n",
    "json_path = \"selected_attributes_Books_reviewerID.json\"  # File input JSONL\n",
    "output_json_path = \"selected_attributes_Books_array_reviewerID.json\"  # Output JSON array\n",
    "\n",
    "# Cek apakah file JSONL ada\n",
    "if not os.path.exists(json_path):\n",
    "    print(f\"❌ File tidak ditemukan: {json_path}\")\n",
    "else:\n",
    "    json_array = []\n",
    "\n",
    "    # Membaca file JSONL dan mengonversi ke format array\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                json_array.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Error decoding JSON: {e}\")\n",
    "\n",
    "    # Menyimpan hasil dalam bentuk array ke file baru\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_array, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ File JSON dalam bentuk array telah disimpan ke {output_json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
