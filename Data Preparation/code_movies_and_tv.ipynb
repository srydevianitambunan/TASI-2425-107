{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3136\\3325442088.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_empty_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3136\\3325442088.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_comma_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris dengan nilai [] di setiap kolom:\n",
      " description    28991\n",
      "title              0\n",
      "asin               0\n",
      "dtype: int64\n",
      "Jumlah baris dengan nilai [, ] di setiap kolom:\n",
      " description    0\n",
      "title          0\n",
      "asin           0\n",
      "dtype: int64\n",
      "                                         description  \\\n",
      "0                                                 []   \n",
      "1                                                 []   \n",
      "2  [Disc 1: Flour Power (Scones; Shortcakes; Sout...   \n",
      "3  [Barefoot Contessa Volume 2: On these three di...   \n",
      "4  [Rise and Swine (Good Eats Vol. 7) includes bo...   \n",
      "\n",
      "                                               title        asin  \n",
      "0                Understanding Seizures and Epilepsy  0000695009  \n",
      "1  Spirit Led&mdash;Moving By Grace In The Holy S...  0000791156  \n",
      "2                  My Fair Pastry (Good Eats Vol. 9)  0000143529  \n",
      "3  Barefoot Contessa (with Ina Garten), Entertain...  0000143588  \n",
      "4                  Rise and Swine (Good Eats Vol. 7)  0000143502  \n"
     ]
    }
   ],
   "source": [
    "# Ukuran chunk\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Buka ulang iterator setiap kali ingin membacanya\n",
    "df1_iter = pd.read_json('filtered_meta_Movies_and_TV.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "for df1_chunk in df1_iter:\n",
    "    # Pastikan kolom hanya berisi list sebelum melakukan applymap\n",
    "    count_empty_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
    "    count_comma_list = df1_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n",
    "\n",
    "    print(\"Jumlah baris dengan nilai [] di setiap kolom:\\n\", count_empty_list)\n",
    "    print(\"Jumlah baris dengan nilai [, ] di setiap kolom:\\n\", count_comma_list)\n",
    "    \n",
    "    # Hentikan setelah satu chunk (hapus jika ingin menghitung semua)\n",
    "    break  \n",
    "\n",
    "print(df1_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris dengan nilai [] di seluruh dataset: 0\n"
     ]
    }
   ],
   "source": [
    "total_empty_list = 0\n",
    "\n",
    "for df1_chunk in df1_iter:\n",
    "    total_empty_list += df1_chunk['description'].apply(lambda x: x == []).sum()\n",
    "\n",
    "print(f\"Total baris dengan nilai [] di seluruh dataset: {total_empty_list}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3136\\571006278.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_empty_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_3136\\571006278.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  count_comma_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris dengan nilai [] di setiap kolom:\n",
      " reviewerID    0\n",
      "asin          0\n",
      "overall       0\n",
      "reviewTime    0\n",
      "dtype: int64\n",
      "Jumlah baris dengan nilai [, ] di setiap kolom:\n",
      " reviewerID    0\n",
      "asin          0\n",
      "overall       0\n",
      "reviewTime    0\n",
      "dtype: int64\n",
      "       reviewerID        asin  overall   reviewTime\n",
      "0  A3478QRKQDOPQ2  0001527665        5  03 11, 2013\n",
      "1  A2VHSG6TZHU1OB  0001527665        5  02 18, 2013\n",
      "2  A23EJWOW1TLENE  0001527665        5  01 17, 2013\n",
      "3  A1KM9FNEJ8Q171  0001527665        5  01 10, 2013\n",
      "4  A38LY2SSHVHRYB  0001527665        4  12 26, 2012\n"
     ]
    }
   ],
   "source": [
    "# Ukuran chunk\n",
    "chunk_size = 1000000  \n",
    "\n",
    "# Buka ulang iterator setiap kali ingin membacanya\n",
    "df2_iter = pd.read_json('filtered_Movies_and_TV.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "for df2_chunk in df2_iter:\n",
    "    # Pastikan kolom hanya berisi list sebelum melakukan applymap\n",
    "    count_empty_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == []).sum()\n",
    "    count_comma_list = df2_chunk.applymap(lambda x: isinstance(x, list) and x == [', ']).sum()\n",
    "\n",
    "    print(\"Jumlah baris dengan nilai [] di setiap kolom:\\n\", count_empty_list)\n",
    "    print(\"Jumlah baris dengan nilai [, ] di setiap kolom:\\n\", count_comma_list)\n",
    "    \n",
    "    # Hentikan setelah satu chunk (hapus jika ingin menghitung semua)\n",
    "    break  \n",
    "\n",
    "print(df2_chunk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File bersih telah disimpan sebagai 'filtered_meta_Movies_and_TV_cleaned.json'\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 1000000  # Sesuaikan dengan kapasitas memori\n",
    "df1_iter = pd.read_json('filtered_meta_Movies_and_TV.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "# File output untuk menyimpan hasil\n",
    "output_file = 'filtered_meta_Movies_and_TV_cleaned.json'\n",
    "\n",
    "# Membuka file untuk menulis hasil bersih\n",
    "with open(output_file, 'w') as f_out:\n",
    "    for df1_chunk in df1_iter:\n",
    "        # Filter baris yang tidak kosong di kolom 'description'\n",
    "        df1_filtered = df1_chunk[df1_chunk['description'].apply(lambda x: x != [])]\n",
    "        \n",
    "        # Simpan ke file JSON (append untuk tiap chunk)\n",
    "        df1_filtered.to_json(f_out, orient='records', lines=True)\n",
    "\n",
    "print(f\"File bersih telah disimpan sebagai '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil merge telah disimpan sebagai 'merge_Movies_and_TV.csv'\n"
     ]
    }
   ],
   "source": [
    "# Membaca file JSON dalam mode chunk untuk efisiensi memori\n",
    "chunk_size = 1000000  \n",
    "df1_iter = pd.read_json('filtered_meta_Movies_and_TV_cleaned.json', lines=True, chunksize=chunk_size)\n",
    "df2_iter = pd.read_json('filtered_Movies_and_TV.json', lines=True, chunksize=chunk_size)\n",
    "\n",
    "# File output untuk hasil merge\n",
    "output_file = 'merge_Movies_and_TV.csv'\n",
    "\n",
    "# Membuka file CSV untuk menulis hasil merge\n",
    "with open(output_file, 'w') as f_out:\n",
    "    header_written = False  # Pastikan header hanya ditulis sekali\n",
    "    \n",
    "    for df1_chunk, df2_chunk in zip(df1_iter, df2_iter):\n",
    "        # Menggabungkan berdasarkan 'asin'\n",
    "        merge_Movies_and_TV = pd.merge(df1_chunk, df2_chunk, on='asin', how='inner')\n",
    "\n",
    "        # Menyimpan ke file CSV (append untuk tiap chunk)\n",
    "        merge_Movies_and_TV.to_csv(f_out, mode='a', index=False, header=not header_written)\n",
    "        header_written = True  # Setelah iterasi pertama, header tidak ditulis lagi\n",
    "\n",
    "print(f\"Hasil merge telah disimpan sebagai '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "1  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "2  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "3  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "4  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "\n",
      "                                               title        asin  \\\n",
      "0  Steve Green: Hide 'em in Your Heart: 13 Bible ...  0001526863   \n",
      "1  Steve Green: Hide 'em in Your Heart: 13 Bible ...  0001526863   \n",
      "2  Steve Green: Hide 'em in Your Heart: 13 Bible ...  0001526863   \n",
      "3  Steve Green: Hide 'em in Your Heart: 13 Bible ...  0001526863   \n",
      "4  Steve Green: Hide 'em in Your Heart: 13 Bible ...  0001526863   \n",
      "\n",
      "       reviewerID  overall   reviewTime  \n",
      "0   AT4UZKGPQ719W        5  05 19, 2009  \n",
      "1  A2DZ09ET7S7ISG        5  07 18, 2007  \n",
      "2  A238RV4L8SVTSF        5  01 19, 2017  \n",
      "3  A3DU3JGKKYJ6Q0        5  12 17, 2016  \n",
      "4   ADJY91Y3KZMYG        5   12 8, 2016  \n"
     ]
    }
   ],
   "source": [
    "# Membaca file CSV hasil merge\n",
    "merge_Movies_and_TV = pd.read_csv('merge_Movies_and_TV.csv')\n",
    "\n",
    "# Menampilkan 5 baris teratas\n",
    "print(merge_Movies_and_TV.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 925636\n"
     ]
    }
   ],
   "source": [
    "# Menghitung jumlah baris\n",
    "num_rows = merge_Movies_and_TV.shape[0]\n",
    "print(\"Jumlah baris:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import base64\n",
    "\n",
    "# Tokenizing 'reviewerID'\n",
    "label_encoder_reviewer = LabelEncoder()\n",
    "merge_Movies_and_TV['reviewerID'] = label_encoder_reviewer.fit_transform(merge_Movies_and_TV['reviewerID'])\n",
    "\n",
    "# Tokenizing 'asin'\n",
    "# Fungsi untuk mengubah asin menjadi format yang mirip dengan sebelumnya\n",
    "def encode_asin(asin):\n",
    "    encoded = base64.b32encode(asin.encode()).decode()[:10]  # Base32 encoding dengan 10 karakter\n",
    "    return encoded\n",
    "\n",
    "# Terapkan encoding ke kolom 'asin'\n",
    "merge_Movies_and_TV['asin'] = merge_Movies_and_TV['asin'].astype(str).apply(encode_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          description  \\\n",
      "0   [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "1   [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "2   [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "3   [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "4   [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "..                                                ...   \n",
      "95   ['An early movie edition of the life of Jesus.']   \n",
      "96   ['An early movie edition of the life of Jesus.']   \n",
      "97   ['An early movie edition of the life of Jesus.']   \n",
      "98   ['An early movie edition of the life of Jesus.']   \n",
      "99   ['An early movie edition of the life of Jesus.']   \n",
      "\n",
      "                                                title        asin  reviewerID  \\\n",
      "0   Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      586297   \n",
      "1   Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      226404   \n",
      "2   Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      178001   \n",
      "3   Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      388575   \n",
      "4   Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      515515   \n",
      "..                                                ...         ...         ...   \n",
      "95                             Where Jesus Walked VHS  GAYDANJQGA      214673   \n",
      "96                             Where Jesus Walked VHS  GAYDANJQGA      500861   \n",
      "97                             Where Jesus Walked VHS  GAYDANJQGA      162366   \n",
      "98                             Where Jesus Walked VHS  GAYDANJQGA      220217   \n",
      "99                             Where Jesus Walked VHS  GAYDANJQGA      111910   \n",
      "\n",
      "    overall   reviewTime  \n",
      "0         5  05 19, 2009  \n",
      "1         5  07 18, 2007  \n",
      "2         5  01 19, 2017  \n",
      "3         5  12 17, 2016  \n",
      "4         5   12 8, 2016  \n",
      "..      ...          ...  \n",
      "95        4  10 27, 2014  \n",
      "96        5  10 19, 2014  \n",
      "97        5  08 17, 2014  \n",
      "98        5   08 6, 2014  \n",
      "99        5   08 4, 2014  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Menampilkan hasil 5 baris pertama\n",
    "print(merge_Movies_and_TV.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menyimpan hasil tokenization ke file baru\n",
    "merge_Movies_and_TV.to_csv('merge_Movies_and_TV_tokenized(2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dengan atribut terpilih telah disimpan sebagai 'Attributes_Movies_and_TV.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "merge_Movies_and_TV = pd.read_csv('merge_Movies_and_TV_tokenized(2).csv')\n",
    "\n",
    "# Pilih atribut tertentu (sesuaikan dengan kebutuhan)\n",
    "selected_columns = ['reviewerID', 'asin', 'title']\n",
    "filtered_df = merge_Movies_and_TV[selected_columns]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "output_file = 'Attributes_Movies_and_TV.csv'\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File dengan atribut terpilih telah disimpan sebagai '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File berhasil dikonversi: Attributes_Movies_and_TV.json\n"
     ]
    }
   ],
   "source": [
    "# Path untuk file input dan output\n",
    "csv_path = 'Attributes_Movies_and_TV.csv'  # Sesuaikan dengan lokasi file\n",
    "json_path = 'Attributes_Movies_and_TV.json'  # Path output JSONL\n",
    "\n",
    "# Membaca CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Menyimpan sebagai JSONL (JSON Lines)\n",
    "df.to_json(json_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"✅ File berhasil dikonversi: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File JSON dalam bentuk array telah disimpan ke Attributes_Movies_and_TV_array.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path file JSON (sesuaikan dengan lokasi file di lokal)\n",
    "json_path = \"Attributes_Movies_and_TV.json\"  # File input JSONL\n",
    "output_json_path = \"Attributes_Movies_and_TV_array.json\"  # Output JSON array\n",
    "\n",
    "# Cek apakah file JSONL ada\n",
    "if not os.path.exists(json_path):\n",
    "    print(f\"❌ File tidak ditemukan: {json_path}\")\n",
    "else:\n",
    "    json_array = []\n",
    "\n",
    "    # Membaca file JSONL dan mengonversi ke format array\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                json_array.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Error decoding JSON: {e}\")\n",
    "\n",
    "    # Menyimpan hasil dalam bentuk array ke file baru\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_array, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ File JSON dalam bentuk array telah disimpan ke {output_json_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 5, Jumlah reviewer unik: 961\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('merge_Movies_and_TV_tokenized(2).csv')\n",
    "\n",
    "# Pilih 2.000 item pertama berdasarkan 'asin' (pastikan unik)\n",
    "selected_asins = df['asin'].drop_duplicates().head(5)\n",
    "\n",
    "# Filter dataset untuk hanya menyertakan review dari 2.000 item tersebut\n",
    "filtered_df = df[df['asin'].isin(selected_asins)]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "filtered_df.to_csv('Dataset_Movies_and_TV_for_analysis(2).csv', index=False)\n",
    "\n",
    "# Menampilkan jumlah unique item dan reviewer\n",
    "num_items = filtered_df['asin'].nunique()\n",
    "num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 5, Jumlah reviewer unik: 5\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('merge_Movies_and_TV_tokenized(2).csv')\n",
    "\n",
    "# Pilih 2.000 item pertama berdasarkan 'asin' (pastikan unik)\n",
    "selected_asins = df['reviewerID'].drop_duplicates().head(5)\n",
    "\n",
    "# Filter dataset untuk hanya menyertakan review dari 2.000 item tersebut\n",
    "filtered_df = df[df['reviewerID'].isin(selected_asins)]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "filtered_df.to_csv('Dataset_Movies_and_TV_for_analysis_ReviewerID.csv', index=False)\n",
    "\n",
    "# Menampilkan jumlah unique item dan reviewer\n",
    "num_items = filtered_df['asin'].nunique()\n",
    "num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         description  \\\n",
      "0  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "1  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "2  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "3  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "4  [\"Steve Green: Hide 'em in Your Heart: 13 Bibl...   \n",
      "\n",
      "                                               title        asin  reviewerID  \\\n",
      "0  Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      586297   \n",
      "1  Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      226404   \n",
      "2  Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      178001   \n",
      "3  Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      388575   \n",
      "4  Steve Green: Hide 'em in Your Heart: 13 Bible ...  GAYDAMJVGI      515515   \n",
      "\n",
      "   overall   reviewTime  \n",
      "0        5  05 19, 2009  \n",
      "1        5  07 18, 2007  \n",
      "2        5  01 19, 2017  \n",
      "3        5  12 17, 2016  \n",
      "4        5   12 8, 2016  \n"
     ]
    }
   ],
   "source": [
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah item unik: 5, Jumlah reviewer unik: 99\n"
     ]
    }
   ],
   "source": [
    "# # Load dataset\n",
    "# df = pd.read_csv('merge_Movies_and_TV_tokenized(2).csv')\n",
    "\n",
    "# # Menghitung jumlah unique reviewerID untuk setiap ASIN\n",
    "# reviewer_counts = df.groupby('asin')['reviewerID'].nunique().reset_index()\n",
    "\n",
    "# # Filter hanya ASIN yang memiliki maksimal 20 reviewer\n",
    "# filtered_asins = reviewer_counts[reviewer_counts['reviewerID'] <= 20]\n",
    "\n",
    "# # Ambil 5 ASIN dengan jumlah reviewer terbanyak dalam batasan ini\n",
    "# top_5_asins = filtered_asins.sort_values(by='reviewerID', ascending=False).head(5)['asin']\n",
    "\n",
    "# # Filter dataset berdasarkan 5 ASIN tersebut\n",
    "# filtered_df = df[df['asin'].isin(top_5_asins)]\n",
    "\n",
    "# # Simpan hasil ke dalam file baru\n",
    "# filtered_df.to_csv('Dataset_Movies_and_TV_for_analysis(2).csv', index=False)\n",
    "\n",
    "# # Menampilkan jumlah unique item dan reviewer\n",
    "# num_items = filtered_df['asin'].nunique()\n",
    "# num_reviewers = filtered_df['reviewerID'].nunique()\n",
    "# print(f\"Jumlah item unik: {num_items}, Jumlah reviewer unik: {num_reviewers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         asin  num_reviewers\n",
      "0  GAYDAMJVGI             42\n",
      "1  GAYDANJQGA             86\n",
      "2  GAYDANJQGE            775\n",
      "3  GAYDANJQHA              7\n",
      "4  GAYDANJRGI             52\n"
     ]
    }
   ],
   "source": [
    "# # Menghitung jumlah unique reviewerID untuk setiap asin\n",
    "# reviewer_counts = filtered_df.groupby('asin')['reviewerID'].nunique().reset_index()\n",
    "\n",
    "# # Ubah nama kolom untuk lebih jelas\n",
    "# reviewer_counts.columns = ['asin', 'num_reviewers']\n",
    "\n",
    "# # Simpan hasil ke dalam file CSV\n",
    "# # reviewer_counts.to_csv('reviewer_count_per_asin.csv', index=False)\n",
    "\n",
    "# # Tampilkan hasil\n",
    "# print(reviewer_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File dengan atribut terpilih telah disimpan sebagai 'selected_attributes_Movies_and_TV_ReviewerID.csv'\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df_analysis = pd.read_csv('Dataset_Movies_and_TV_for_analysis_ReviewerID.csv')\n",
    "\n",
    "# Pilih atribut tertentu (sesuaikan dengan kebutuhan)\n",
    "selected_columns = ['reviewerID', 'asin', 'title']\n",
    "filtered_df = df_analysis[selected_columns]\n",
    "\n",
    "# Simpan hasil ke dalam file baru\n",
    "output_file = 'selected_attributes_Movies_and_TV_ReviewerID.csv'\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"File dengan atribut terpilih telah disimpan sebagai '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File berhasil dikonversi: selected_attributes_Movies_and_TV_ReviewerID.json\n"
     ]
    }
   ],
   "source": [
    "# Path untuk file input dan output\n",
    "csv_path = 'selected_attributes_Movies_and_TV_ReviewerID.csv'  # Sesuaikan dengan lokasi file\n",
    "json_path = 'selected_attributes_Movies_and_TV_ReviewerID.json'  # Path output JSONL\n",
    "\n",
    "# Membaca CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Menyimpan sebagai JSONL (JSON Lines)\n",
    "df.to_json(json_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(f\"✅ File berhasil dikonversi: {json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ File JSON dalam bentuk array telah disimpan ke selected_attributes_Movies_and_TV_ReviewerID_array.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path file JSON (sesuaikan dengan lokasi file di lokal)\n",
    "json_path = \"selected_attributes_Movies_and_TV_ReviewerID.json\"  # File input JSONL\n",
    "output_json_path = \"selected_attributes_Movies_and_TV_ReviewerID_array.json\"  # Output JSON array\n",
    "\n",
    "# Cek apakah file JSONL ada\n",
    "if not os.path.exists(json_path):\n",
    "    print(f\"❌ File tidak ditemukan: {json_path}\")\n",
    "else:\n",
    "    json_array = []\n",
    "\n",
    "    # Membaca file JSONL dan mengonversi ke format array\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                json_array.append(json.loads(line.strip()))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️ Error decoding JSON: {e}\")\n",
    "\n",
    "    # Menyimpan hasil dalam bentuk array ke file baru\n",
    "    with open(output_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(json_array, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ File JSON dalam bentuk array telah disimpan ke {output_json_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
